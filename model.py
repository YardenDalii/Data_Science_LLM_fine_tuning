from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFaceHub
from langchain_huggingface import HuggingFaceEndpoint
from langchain.chains import retrieval_qa, RetrievalQA
import gradio as gr
from gradio.themes.base import Base
from data_scraping import MONGODB_URI, EMBEDDING_MODEL, MODEL_NAME, HF_TOKEN, database_connection, ask_model

from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

def truncate_context(documents, max_tokens):
    total_tokens = 0
    truncated_docs = []
    
    for doc in documents:
        doc_tokens = len(doc.page_content.split())
        if total_tokens + doc_tokens > max_tokens:
            break
        truncated_docs.append(doc)
        total_tokens += doc_tokens
    
    return truncated_docs

collection = database_connection(MONGODB_URI)

embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)


template = """Answer the following question based on the provided context:
    <context>
    {context}
    </context>

    Question: {input}
    """

llm = HuggingFaceEndpoint(repo_id=MODEL_NAME, huggingfacehub_api_token=HF_TOKEN, max_new_tokens=128)

prompt = ChatPromptTemplate.from_template(template)
document_chain = create_stuff_documents_chain(llm, prompt)

question = "What were the main events during the October 7th massacre?"

response, documents = ask_model(question, embeddings=embeddings, top_k=20)

max_context_tokens = 500
truncated_documents = truncate_context(documents, max_context_tokens)

answer = document_chain.invoke({"input": question, "context": truncated_documents})
print(answer)


# vectorStore= MongoDBAtlasVectorSearch(collection, embeddings)


# def query_data(query):
#     docs = vectorStore.similarity_search(query, k=1)

#     if docs:
#         as_output = docs[0].page_content
#     else:
#         as_output = "No relevant documents found."
#     llm = HuggingFaceEndpoint(repo_id=MODEL_NAME, huggingfacehub_api_token=HF_TOKEN)
#     # llm = HuggingFaceHub(repo_id=MODEL_NAME, huggingfacehub_api_token=HF_TOKEN)
#     retriever = vectorStore.as_retriever()
#     # qa = retrieval_qa.from_chain_type(llm, chain_type="stuff", retriever=retriever)
#     qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
#     print(qa)
#     retriever_output = qa({"query": query}) if docs else "No relevant documents found for generating an answer."

#     return as_output, retriever_output

# result, _ = query_data("What were the main events during the october 7th masecure?")
# print(result)


# with gr.Blocks(theme=Base(), title="Question Answering App using Vector Search + RAG") as demo:
#     gr.Markdown(
#         """
# # Question Asnwering App using Atlas Vector Search + RAG Architecture
# # """)
#     textbox = gr.Textbox(label="Enter your question:")
#     with gr.Row():
#         button = gr.Button("Submit", variant="primary")
#     with gr.Column():
#         output1 = gr.Textbox(lines=1, max_lines=10, label="Output with just Atlas Vector Search (return text)")
#         output2 = gr.Textbox(lines=1, max_lines=10, label="output generated by chaining Atlas Vector Search")

#     button.click(query_data, textbox, outputs=[output1, output2])

# demo.launch()


# What were the main events during the october 7th masecure?
